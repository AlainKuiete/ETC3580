---
title: 'Week 4: Solutions'
author: "ETC3580"
output:
  html_document:
    fig_height: 5
    fig_width: 8
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: false
    number_sections: false
    code_folding: show
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, warning=FALSE, message=FALSE)
options(width=80,digits=3)
```

```{r loadpackages}
library(faraway)
library(tidyverse)
library(visreg)
library(broom)
```

# Exercise 4A

## A2(a)

Create the proportion and plot the data:

```{r}
turtle <- turtle %>%
  as_tibble() %>%
  mutate(proportion = male/(male + female))

turtle %>%
  ggplot(aes(x=temp, y=proportion)) +
    geom_point() + xlab("Temperature") + ylab("Proportion Male")
```

We see a nonlinear (possibly quadratic) relationship.

## A2(b)

```{r}
lmod <- glm(cbind(male, female) ~ temp, family=binomial, turtle)
summary(lmod)
```

We can check the fit of the model:

```{r}
1-pchisq(deviance(lmod), df.residual(lmod))
```

No - it does not fit.

## A2(c)

We see that all the covariate classes are at least size 5

```{r}
turtle %>% mutate(total = male+female) %>% select(total)
```

So no sparseness.

## A2(d)

Check for outliers. Unfortunately `broom::augment()` does not work with binomial responses, so we will use `residual()` instead.

```{r}
tibble(.resid = residuals(lmod)) %>%
  ggplot(aes(sample=.resid)) +
    geom_qq()
```

No outliers are apparent.

## A2(e)

Compute and plot the empirical logits:

```{r}
turtle %>%
  mutate(elogit = log((male+0.5)/(female+0.5))) %>%
  ggplot(aes(x=temp, y=elogit)) + geom_point()
```

There is some evidence of non-linearity.

## A2(f)

We add a quadratic term to the model:

```{r}
lmod2 <- glm(cbind(male, female) ~ temp + I(temp^2),
             family=binomial, turtle)
summary(lmod2)
anova(lmod, lmod2, test="Chisq")
```

We see that the quadratic term is significant. We check the fit of the model:

```{r}
1-pchisq(deviance(lmod2),df.residual(lmod2))
```

We see that the model is acceptable (but only just).

Finally, let's look at the fitted proportions along with the actual proportions

```{r}
visreg(lmod2, "temp", scale='response', gg=TRUE) +
  theme_bw() +
  geom_point(data=turtle, aes(x=temp, y=proportion)) +
    geom_point() + xlab("Temperature") + ylab("Proportion Male")
```

It does not look non-linear enough. The observations for the lowest temp are all too low, and the observations for the next temp are all too high. We probably need something more non-linear than quadratic.

## A2(g)

Compute the summary statistics within the replicate groups:

```{r}
turtle_totals <- turtle %>%
  group_by(temp) %>%
  summarise(male=sum(male), female=sum(female), sd=sd(proportion))
turtle_totals
```

Derive the total and proportion and use these to produce the expected SD. The sample size for each case is not exactly a third of the total for the replicate group but it seems a reasonable approximation.

```{r}
turtle_totals %>%
  mutate(total=male+female,
         proportion=male/total,
         expectedSD=sqrt(proportion*(1-proportion)/(total/3)))
```

The observed SDs are less than expected except for the third group. With only five observations, we cannot make a strong conclusion but there is no convincing evidence
of a difference.

## A2(h)

```{r}
glmod <- glm(cbind(male, female) ~ temp, family=binomial, turtle_totals)
summary(glmod)
```

We see that coefficients and standard errors are the same but the sample size and deviances are different (although not the difference in deviance).


# Exercise 4B

```{r gavote}
faraway::gavote %>%
  as.tibble() %>%
  rename(usage = rural) %>%
  mutate(
    undercount = (ballots - votes) / ballots,
    pergore = gore / votes,
    county = rownames(faraway::gavote)
  ) ->
  gavote
lmod1 <- lm(undercount ~ perAA*(pergore + equip) + econ + pergore*usage,
           data=gavote)
summary(lmod1)
```

However, `undercount` is a proportion, so can only take values on [0,1].

## B1

```{r B1}
lmod2 <- glm(undercount ~ perAA*(pergore + equip) + econ + pergore*usage,
           family=quasibinomial, data=gavote)
summary(lmod2)
```

## B2

The coefficients are in the same direction and relative magnitude, but the actual values are very different. This is because of the logistic transformation.

## B3

```{r B3}
visreg(lmod1, "econ", gg=TRUE, scale='response') + theme_bw()
visreg(lmod2, "econ", gg=TRUE, scale='response') + theme_bw()
visreg(lmod1, "pergore", by="usage", gg=TRUE, scale='response') + theme_bw()
visreg(lmod2, "pergore", by="usage", gg=TRUE, scale='response') + theme_bw()
visreg(lmod1, "perAA", by="equip", gg=TRUE, scale='response') + theme_bw()
visreg(lmod2, "perAA", by="equip", gg=TRUE, scale='response') + theme_bw()
visreg2d(lmod1, "perAA", "pergore", scale='response')
visreg2d(lmod2, "perAA", "pergore", scale='response')
```

The conditional effects are very similar. So the resulting model predictions are almost identical. One noticeable difference is when the linear model predictions go negative (in the plot of undercount vs perAA | equip=LEVER), while the binomial proportion model forces the effects to stay positive.

## B4

The Gaussian assumption has little effect. Even though some of the undercount proportions are 0 or close to 0, there are enough observations away from the zero boundary to allow the Gaussian model to work reasonably well.

## B5

```{r B5}
library(mgcv)
lmod3 <- gam(undercount ~ perAA*(pergore + equip) + econ + pergore*usage,
           data=gavote, family=betar())
summary(lmod3)
```

The coefficients are in the same direction and relative magnitude, but the actual values are very different. This is because the beta distribution has a different link function from the quasiBinomial model.

```{r}
visreg(lmod3, "econ", gg=TRUE, scale='response') + theme_bw()
visreg(lmod3, "pergore", by="usage", gg=TRUE, scale='response') + theme_bw()
visreg(lmod3, "perAA", by="equip", gg=TRUE, scale='response') + theme_bw()
visreg2d(lmod3, "perAA", "pergore", scale='response')
```

The conditional effects are quite different now, which is surprising because both the quasiBinomial and the Beta model are both using a logit link function. Presumably this is due to a different optimization criterion.

We could compare their predictions (as we did for the mammal sleep example):


```{r}
# Compare predictions
pred1 <- fitted(lmod1)
pred2 <- fitted(lmod2, type='response')
pred3 <- fitted(lmod3, type='response')
gavote %>%
  ggplot(aes(x=undercount)) +
    geom_point(aes(y=pred1, col="Linear")) +
    geom_point(aes(y=pred2, col="QuasiBinomial")) +
    geom_point(aes(y=pred3, col="Beta")) +
    ylab("Fitted") + geom_abline(intercept=0, slope=1)
```

The quasiBinomial and Linear values (blue and green) seem much closer than the Beta values (pink). Let's compare differences with linear predictions.

```{r}
gavote %>%
  ggplot(aes(x=undercount)) +
    geom_point(aes(y=pred2-pred1, col="QuasiBinomial - Linear")) +
    geom_point(aes(y=pred3-pred1, col="Beta - Linear")) +
    ylab("Difference in predictions")
```

As expected, the Beta values are much more different than the other two sets of values.

## B6

The logitNormal can't be used here because there are counties with zero undercount.

```{r}
gavote %>% 
  select(county, undercount) %>%
  arrange(undercount)
```

Of course, we could remove them and fit a model to the remaining data.
